\section{Huffman codes}

\subsection{The Problem}

Consider this problem - you have a message - suppose 

\begin{center}
\texttt{I cannot meet you today. Lets meet tomorrow. - Jamie}
\end{center}

and you want to send this to your friend. 
Unfortunately, the cost to send messages is expensive (think of the mobile phone fees...) - how do you minimize the cost of sending this message given that you can tell your friend its encoding in advance.
Naturally, computers store data in binary form - \texttt{001011....} so it would be ideal to encode this string to binary. But, how do you find an optimal encoding - one that takes the least space.

Another problem is ambiguity. Suppose you have a message \texttt{hello world}, and suppose that your encoding maps \texttt{h} to \texttt{0} and \texttt{e} to \texttt{001}. and \texttt{1} to the character \texttt{o}. 
Hence our message would be encoded as 
\texttt{0001...}

Unfortunately, given this code alone, we cannot differentiate between \texttt{hhho...} and \texttt{he...} as both of them produces the same binary output. 

\subsection{Removing ambiguity}

Consider above that if we see the code \texttt{0..}, this can either mean the character \texttt{h} or some other character that also starts with \texttt{0}. We want to remove this ambiguity by adding the following constraint:

\begin{define}
    Let $\Sigma \neq \emptyset$ the set of characters in our message, let $\phi: \Sigma \to \cbr{0, 1}^*$ our encoding where $\cbr{0, 1}^*$ is the set of all binary messages. 
    Then $\phi$ is called ``prefix-free'' if for all $a, b \in \Sigma$ such that $a \neq b$, $\phi(a)$ is not a prefix of $\phi(b)$ (and vice versa... since $a, b$ are arbitrary)
\end{define}

Now, if we see some coded message \texttt{*****010.....} and we know that \texttt{*****} is mapped to a particular character, then our message contains that character since no other character can be encoded beginning with \texttt{*****} due to the above definition. Thus, we remove ambiguity and can iterative decode message based on the shortest encoding match. 

\subsection{Constructing a tree}

If we have an encoding, one thing we can do is construct an encoding binary tree where we start at the root, and we move left or right depending if our first character is \texttt{0} or \texttt{1}, and then repeat with the next character. 
Once we find a node with a character, we stop and determine that is the character we want. 

\begin{example}
If we have $\Sigma=\cbr{a,b,c,d,e}$, and our $\phi$ maps $\Sigma$ to

\[
    x \in \Sigma \mapsto \begin{cases}
        101, & x=a \\
        100, & x=b \\
        0, & x=c \\
        110, & x=d \\
        111, & x=e \\
    \end{cases}
\],

our tree would look as

\begin{center}
\end{center}

\end{example}

Notice that in a prefix-free encoding, all character nodes are leaf nodes. 
This is because if there is a leaf node with another leaf node descendant, this means that the two characters share a prefix which violates our principle. 
We can now formally define the problem. The total access cost is the access time of all characters. This is the sum of each character's number of occurrence - denoted $C_s(c)$ times the cost of each character which is the depth of this character in the tree denoted $D_T(c)$

\[
    \mathrm{Cost} \defeq \sum_{c \in \Sigma} C_s(c) D_T(c)  
\]

Moreover, we normalize this value by dividing over the number of characters - thus yielding the \textit{average} access time per character. Hence, 

\[
    \text{Average cost} = \sum_{c \in \Sigma} \frac{C_s(c)}{C_s} D_T(c)  
\]

where $C_s$ is the number of characters in string $s$. We then call the fraction of $c$'s occurrence in sentence $s$ over the total number of characters the ``frequency'' of c. 
In other words, 

\[
    \text{c's Frequency, c.freq} \defeq \frac{C_s(c)}{C_s}.
\]

\subsection{The greedy algorithm}

Observe that for an optional $T$ - that is, the lowest average cost, any leaf node must have a sibling. This is because if not, let $\ell$ the character that does not have a sibling, we can construct a new tree $T'$ with $\ell$ in the place of its parent node in $T$. 
In this case, $\ell$'s depth is $1$ less than that of $T$ and 

\[
\begin{split}
  \text{Cost}(T)-\text{Cost}(T') &= \sum_{c \in \Sigma} \text{c.freq}\cdot\del{ D_T(c)-D_{T'}(c)} \\
  &=\ell.\text{freq}(D_T(\ell)-D_{T'}(\ell)) \\
  &=\ell.\text{freq} \\
  &>0
\end{split}
\]

which is a contradiction. 
Hence, $T$'s leaves must always have a sibling.
Intuitively, if we have a leaf without sibling, we are allocating \texttt{x...x1} to a character $\ell$ without having \texttt{x...x0} allocated (or vice versa). 
By the prefix rule, the encoding \texttt{x...x} has no mapping and could be used for $\ell$ instead, reducing access time. 
Note that here we assume $\ell$ is used in string $s$, otherwise we could just discard $\ell$ from $\Sigma$. 

Another observation from the above is that for the leaf $c$ with the deepest depth (i.e. deepest down in the tree), there must be a sibling with the same depth ($c'$) (because $c$ has the highest depth). 
Ideally, we should allocate the two least used characters into this two slot (because if not, there has to be a node with highest depth $(c, d)$ and we could replace $(c, d)$ with the two least frequent characters) and lower the access cost. 
Hence there exists an optional solution with our greedy choice (the character least used). 

The question now is how do we reduce the problem. We know that $\ell$ and $\ell'$ are differentiated by the last bit with some common prefix. 
We have to determine this common prefix. 
Note that this common prefix is itself an encoding, and that the decoding algorithm can reach this prefix when the character is either $\ell$ or $\ell'$. 
Hence, if we reduce $\ell$ and $\ell'$ to a common character, let's say $\lambda$, for any solution we find with this reduced $\Sigma'$, we can subsitute $(\ell, \ell')$ back for $\lambda$, we get the our solution.

Moreover, if the optimal solution for $\Sigma'$ is not a subset of $\Sigma$'s optimal solution - i.e. $\Sigma'$' solution produces a higher cost than $\Sigma$ solution with $(\ell, \ell')$ replaced, then we can take the solution for $\Sigma$, replace $(\ell, \ell')$ with $\lambda$ and get a lower cost solution - contradiction. With this, we rinse and repeat until we are down to two or one characters, which we stop. 

As a formal algorithm, 

\begin{algorithm}[H]
\caption{Iterative Huffman Algorithm}
\label{greedy-huffman}
\begin{algorithmic}[1]
    \Require $s$ a string with characters in $\Sigma$, with all characters used. 
    \Function{Huffman}{$s, \Sigma$}
        \State $Q \gets$ \text{new min-Priority queue}
        \ForAll{$c \in \Sigma$}
            \State $T \gets$ new tree
            \State $T.\text{root} \gets c$
            \State $Q.\Call{Insert}{T,c.\text{freq}}$
        \EndFor

        \While{$\abs{Q}>1$}
            \State $T_1 \gets Q.\Call{ExtractMin}{}$ \Comment{Represents $\ell$}
            \State $T_2 \gets Q.\Call{ExtractMin}{}$ \Comment{Represents $\ell'$}
            \State $T \gets$ new tree with $T_1$ on its left node, $T_2$ on the right node 
                \Comment{T's root has no label; this is to represent a non-leaf node.}
            \State $Q.\Call{Insert}{T,T_1.\text{freq}+T_2.\text{freq}}$ \Comment {Represents $\lambda$, a combined character for $(\ell, \ell')$}
        \EndWhile
        \State \Return $Q$'s one tree. \Comment{This tree in $Q$ represents the optimal encoding}
    \EndFunction
\end{algorithmic}
\end{algorithm}

To prove Algorithm \ref{greedy-huffman}, we prove two lemmas first:

\begin{lemma}
    Given alphabet set $\Sigma$ with frequency derived from some string, there exists an optimal encoding binary tree such that the two deepest siblings are labeled by the two least frequent characters.
\end{lemma}

\begin{proof}
    Let $T$ an optimal encoding binary tree for $\Sigma$.
    Let $\ell, \ell' \in \Sigma$ the two deepest siblings (they come in pairs from the above).
    Without loss of generality, $\ell.\text{freq} \le \ell'.\text{freq}$
    Let $x, y \in \Sigma$ the two characters with the least frequency. 
    Without loss of generality again, suppose that $x.\text{freq} \le y.\text{freq}$.
    If $x=\ell$ and $y=\ell'$ or vice versa, we are done (or have the same frequency).

    Otherwise, suppose that $x\neq\ell'$. If $x = \ell$, proceed with the argument below but with $y$ and $\ell$'s reversed.
    Let $T'$ a new tree the same as $T$ but with $x$ swapped with $\ell$ 
    Then, 

    \[
    \begin{split}
    \text{Cost}(T',\Sigma)-\text{Cost}(T,\Sigma) &= \sum_{c \in \Sigma} c.\text{freq}\cdot(D_{T'}(c) - D_T(c)) \\
    &= x.\text{freq}(D_{T'}(x)-D_{T}(x)) + \ell.\text{freq}(D_{T'}(\ell) - D_{T}(\ell)) \\
    &= x.\text{freq}(D_{T'}(x)-D_{T}(x)) + \ell.\text{freq}(D_{T}(x)- D_{T'}(x)) \\
    &= \underbrace{(x.\text{freq} - \ell.\text{freq})}_{\le 0} \underbrace{(D_{T'}(x)-D_{T}(x))}_{\ge 0} \\
    &\le 0.
    \end{split}
    \]

    Hence $\text{Cost}(T',\Sigma) \le \text{Cost}(T,\Sigma)$.
    But since $T$ is an optimal tree, $\text{Cost}(T',\Sigma)$ is at least $\text{Cost}(T,\Sigma)$. 
    Hence $T$ and $T'$ has the same cost and are both optimal.
    If $y \neq \ell'$ (in other words, $y.\text{freq} \le \ell'.\text{freq}$), repeat the procedure above with $y$ and $\ell'$. 
\end{proof}

\begin{lemma}
    Let $T$ an optimal binary tree for $\Sigma$ like above. 
    Let $x, y$ two sibling leaves in $T$ and let $\Sigma'$ the alphabet set defined by removing $x, y$ from $\Sigma$ and adding a character $\sigma$ to represent ``either $x$ or $y$'' with frequency as $x$ and $y$ combined.
    In other words, 

    \[
        \Sigma' \defeq \del{\Sigma \setminus \cbr{x, y}} \cup \cbr{\sigma}, \quad \sigma.\text{freq} 
        \defeq x.\text{freq} + y.\text{freq}
    \]

    Let $T'$ the tree $T$ but with nodes $x$ and $y$ removed and their parent labeled $\sigma$. Then 
    $T'$ is an optimal tree for $\Sigma'$. 
\end{lemma}

\begin{proof}
    Let $\Sigma, \Sigma'$ as above. Let $S'$ an optimal tree for $\Sigma'$ and suppose that 

    \[
        \text{Cost}(S',\Sigma')<\text{Cost}(T',\Sigma'). 
    \]

    Let $S$ an encoding tree for $\Sigma$ by adding $x, y$ as child of $\sigma$ (and removing the $\sigma$ label in process). 
    Then $\text{Cost}(S,\Sigma)-\text{Cost}(T,\Sigma)$ is 

    \[
        \sum_{c \in \Sigma \setminus \cbr{x, y}} c.\text{freq}(D_S(c)-D_T(c)) + x.\text{freq}(D_S(x)-D_T(x))+y.\text{freq}(D_S(y)-D_T(y))
    \]

    The first term is 

    \[
        \text{Cost}(S,\Sigma)-\text{Cost}(T,\Sigma) - \sigma.\text{freq}(D_{S'}(\sigma) - D_{T'}(\sigma)) 
    \]

    Note that 

    \[
        D_{S'}(\sigma) = D_{S}(x) - 1 = D_{S}(y) - 1
    \]

    and 

    \[
        D_{T'}(\sigma) = D_{T}(x) - 1 = D_{T}(y) - 1
    \]

    and since $\sigma.\text{freq} = x.\text{freq} + y.\text{freq}$, by distributive property, $\text{Cost}(S,\Sigma)-\text{Cost}(T,\Sigma)$ is $\text{Cost}(S',\Sigma')-\text{Cost}(T',\Sigma')$ plus

    \[
        x.\text{freq}(D_S(x)-D_T(x) - D_{S'}(\sigma) + D_{T'}(\sigma)) +y.\text{freq}(D_S(y)-D_T(y)-D_{S'}(\sigma) + D_{T'}(\sigma))
    \]

    All the depth terms cancel out and hence is $0$. Hence, 

    \[
        \text{Cost}(S,\Sigma)< \text{Cost}(T,\Sigma)
    \]

    which is a contradiction since $T$ is already an optimal tree. 
\end{proof}

With this, we can prove that this algorithm is correct.

\begin{theorem}
    Algorithm \ref{greedy-huffman} is correct - that is, it produces an optimal encoding tree. 
\end{theorem}

\begin{proof}
    Consider the loop invariant defined as: \textit{For each iteration of line 7, there exists an optimal encoding tree with each subtree in $Q$ part of this tree}.
    Initially, the invariant vacously holds since we only have a singleton leaf.
    For the maintainance, let $T$ be the current optimal tree that contains all the subtrees in $Q$. 

    Iteratively, we can see each element in $Q$ as a leaf node. 
    Then since we are taking the two least frequent characters in $Q$, there exists an optimal tree such that these two are siblings - call this tree $S$.
    Then, $S$ with all leaves replaced with the subtree is also an optimal tree for $\Sigma$ as if it is not (i.e. $R$ a tree for $\Sigma$ with a lower cost, then we see that we can construct a lower cost subtree $R'$ with each leaf element in $Q$).

    Hence there exists an optimal encoding tree with these two subtrees combined. Trivially, this loop terminates since $\abs{Q}$ decreases by one for each iteration. And when it terminates, there is only one tree left which means all elements are connected and this is the encoding tree.
    Note that the last sentence uses the invariant that all subtrees are connected which can be easily proved. 
\end{proof}

We notice that the structure of Huffman problem is quite similar to Kruskal's algorithm where there exists an optimal solution for every greedy choice we picked which leads us to the final answer.
We can generalize this to an abstract model - Matroids.

\section{Kruskal's MST Algorithm}

See CMPUT 204 Slides.

\section{Matroids}

With the two problems in mind above, can we create a model with sets of solutions that can be iteratively constructed by picking a greedy option? 
The answer is yes. 

\begin{define}
    A ``matroid'' $\mathcal{M}$ is two sets - $(U, \mathcal{I})$ such that 

    \begin{enumerate}
        \item $U$, the ``universe'' set, is finite. That is, $\abs{U} \in \naturals_0$. 
        \item $\mathcal{I} \subset \mathcal{P}(U)$ where $\mathcal{P}(U)$ is the power set of $U$. 
        In other words, $\mathcal{I}$ is a collection of subsets of $U$ such that 
        \begin{enumerate}
            \item \textit{Hereditary Property.} For each $S \in \mathcal{I}$, if $R \subset S$, then $R \in \mathcal{I}$.
            \item \textit{Augmentation Property.} For all $A, B \in \mathcal{I}$ such that $\abs{A} > \abs{B}$, there exists some $x \in A \setminus B$ such that 

            \[
                B \cup \cbr{x} \in \mathcal{I}. 
            \]
        \end{enumerate}
    \end{enumerate}
\end{define}

\begin{remark}
    This means that for any matroid $\mathcal{M}=(U,\mathcal{I})$, we have that $\emptyset \in \mathcal{I}$. 
\end{remark}

\begin{example}
\begin{enumerate}
    \item Let $U \defeq \cbr{1, ..., n}$ for $n \in \naturals$ and let $\mathcal{I} \defeq \powerset\del{U}$. Then $(U, \mathcal{I})$ trivially forms a matroid.
    \item On the more extreme size, let $\mathcal{I}' = \cbr{\emptyset}$. Then $U$ along with $\mathcal{I}'$ too forms a matroid since we can both ``ignore'' the two requirements for $\mathcal{I}'$. 
    \item More generally, let

    \[
        \mathcal{I} \defeq \cbr{A \subset U : \abs{A} \le k}, \quad 0 \le k \le \abs{U}
    \]

    Then $\mathcal{I}$ too is a matroid alongside $U$. 

    \item As a nontrivial example, let $n \ge 3$ and 

    \[
        \mathcal{I} \defeq \cbr{\emptyset, \cbr{1}, \cbr{2}, \cbr{3}, \cbr{1,2}, \cbr{2,3}}.
    \]

    We can see that the Hereditary property is satisfied and that for the empty set, we can fill a singleton element $1, 2, 3$ to make it a matroid. 
    Otherwise, for singleton sets, we can add either $1,2, 3$ to make it into a two-element set. 
    \item If $\cbr{1, 2, 3} \in \mathcal{I}$ as the above, $\mathcal{I}$ does not form a matroid because $\cbr{1, 3} \notin \mathcal{I}$. 
\end{enumerate}
\end{example}

To properly formulate the greedy problem, we need the notion of maximality - i.e. the upper limit to the solution. 

\begin{define}
    Let $\mathcal{M}=(U, \mathcal{I})$ matroid, then we call set $A \in \mathcal{I}$ ``maximal'' if for any $x \notin A$, $A \cup \cbr{x} \notin \mathcal{I}$. In other words, we cannot add any more element to $A$. 

    We also call $A$ ``maximum'' if $A$ has the biggest cardinatlity in $\mathcal{I}$. In other words, 

    \[
        \max_{S \in \mathcal{I}} \abs{S} = \abs{A}. 
    \]
\end{define}

We can formulate one lemma:

\begin{lemma}
    Let $\mathcal{M}=(U,\mathcal{I})$ a matroid. 
    Then $A \in \mathcal{I}$ is maximal if and only if $A$ is maximum. 
\end{lemma}

\begin{proof}
    \toquotation Let $A$ maximal. If $A$ is not maximum, let $A'$ such that $\abs{A'} > \abs{A}$. 
    By the augmentation property, there is some $x \in A' \setminus A$ such that $A \cup \cbr{x} \in \mathcal{I}$. 
    Contradiction to the maximal property.

    \fromquotation If $A$ is maximum, trivially we cannot add any more element to $A$ such that it remains in $\mathcal{I}$. Otherwise $A$ is not maximum. 
\end{proof}

Moreover, if each element in $U$ have a weight denoted by function $w: U \to \real_{\ge 0}$, we can use the greedy algorithm to find a maximal set $I$ with the highest weight.

\begin{algorithm}[H]
\caption{General greedy algorithm for a matroid}
\label{greedy-matroid}
\begin{algorithmic}[1]
    \Require $\mathcal{M}$ a matroid and $w: U \to \real_{\ge 0}$ weight function. 
    \Ensure $S$ is an maximal set with weight maximized. 
    \Function{GreedyMatroid}{$\mathcal{M}=(U,\mathcal{I}), w$}
        \State $S \gets \emptyset$
        \State Sort $U$ in nonincreasing order by $w(u)$ weight for $u \in U$ \Comment In other words, equal or decreasing.
        \ForAll{$x \in U$ in this order}
            \If{$S \cup \cbr{x} \in \mathcal{I}$}
                \State $S \gets S \cup \cbr{x}$
            \EndIf
        \EndFor
        \State \Return $S$ 
    \EndFunction
\end{algorithmic}
\end{algorithm}

If $\abs{U}=n$ and the runtime to check if $A \in \mathcal{I}$ is $O(f(n))$ for some $f: \naturals \to \real_{\ge 0}$, then the overall runtime of this algorithm is $O(n\log n + nf(n))$. 
Of course, if $U$ is already sorted, the runtime is reduced to $O(nf(n))$, and if $f(n)$ is bounded, the runtime is simply $O(n)$. 